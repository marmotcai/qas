{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitqascondaaa84361cb93b4ef082d52ef8c2f05dee",
   "display_name": "Python 3.7.6 64-bit ('qas': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([256, 5, 512])\ntensor([[1, 2, 3, 1, 2, 3, 1, 2, 3]])\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\ntensor([[1., 2.],\n        [3., 4.]])\ntensor(7.5000)\n"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "a = torch.Tensor(128, 1, 512)\n",
    "b = a.repeat(2, 5, 1)\n",
    "print(b.shape)\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "xnew = x.repeat(1,3)\n",
    "print(xnew)\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "xnew = x.repeat(3,1)\n",
    "print(xnew)\n",
    "\n",
    "tensor = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(tensor)\n",
    "\n",
    "tensor_mean = torch.mean(tensor * tensor)\n",
    "print(tensor_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x.size:  torch.Size([2, 3, 5])\nx.permute(2, 0, 1).size():  torch.Size([5, 2, 3])\ntensor([[[1, 2, 3],\n         [4, 5, 6]]])\nunpermuted.size:  torch.Size([1, 2, 3])\npermuted.size:  torch.Size([3, 1, 2])\nview_test.size:  torch.Size([1, 3, 2])\n"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 5) \n",
    "print('x.size: ', x.size()) \n",
    "\n",
    "print('x.permute(2, 0, 1).size(): ', x.permute(2, 0, 1).size())\n",
    "\n",
    "a=np.array([[[1,2,3],[4,5,6]]])\n",
    "unpermuted=torch.tensor(a)\n",
    "print(unpermuted)\n",
    "print('unpermuted.size: ', unpermuted.size())     #  ——>  torch.Size([1, 2, 3])\n",
    "\n",
    "permuted=unpermuted.permute(2,0,1)\n",
    "print('permuted.size: ', permuted.size())         #  ——>  torch.Size([3, 1, 2])\n",
    "\n",
    "view_test = unpermuted.view(1,3,2)\n",
    "print('view_test.size: ', view_test.size())     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.6614,  0.2669,  0.0617],\n        [ 0.6213, -0.4519, -0.1661]])\ntensor([[-1.5228,  0.3817, -1.0276]])\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.6614,  0.2669,  0.0617],\n        [ 0.6213, -0.4519, -0.1661],\n        [-1.5228,  0.3817, -1.0276]])"
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "\n",
    "y = torch.randn(1,3)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "torch.cat((x,y),0)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\ntensor([[1., 1., 1., 2., 2., 2., 2.],\n        [1., 1., 1., 2., 2., 2., 2.]])\ntensor([[1., 1., 1., 2., 2., 2., 2.],\n        [1., 1., 1., 2., 2., 2., 2.]])\n"
    }
   ],
   "source": [
    "A = torch.ones(2,3)\n",
    "B = 2*torch.ones(4,3)#4x3的张量（矩阵）\n",
    "C=torch.cat((A,B), 0)\n",
    "print(C)\n",
    "D = 2*torch.ones(2,4) #2x4的张量（矩阵）\n",
    "C = torch.cat((A,D), 1)\n",
    "print(C)\n",
    "C = torch.cat((A,D), dim=1)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 2.],\n        [3., 4.]], requires_grad=True)\ntensor(7.5000, grad_fn=<MeanBackward0>)\n"
    }
   ],
   "source": [
    "# 定义Variable, requires_grad用来指定是否需要计算梯度\n",
    "variable = Variable(tensor, requires_grad = True)\n",
    "print(variable)\n",
    "\n",
    "# 计算x^2的均值\n",
    "variable_mean = torch.mean(variable * variable)\n",
    "print(variable_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.5000, 1.0000],\n        [1.5000, 2.0000]])\n"
    }
   ],
   "source": [
    "# variable进行反向传播\n",
    "# 梯度计算如下：\n",
    "# variable_mean = 1/4 * sum(variable * variable)\n",
    "# d(variable_mean)/d(variable) = 1/4 * 2 * variable = 1/2 * variable\n",
    "variable_mean.backward()\n",
    "\n",
    "# 输出variable中的梯度\n",
    "print(variable.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 1.,  4.],\n        [ 9., 16.]])\ntensor([[ 1.,  4.],\n        [ 9., 16.]], grad_fn=<MulBackward0>)\ntensor([[1., 2.],\n        [3., 4.]])\n"
    }
   ],
   "source": [
    "# *表示逐元素点乘,不是矩阵乘法\n",
    "print(tensor * tensor)\n",
    "print(variable * variable)\n",
    "print(variable.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "张量的类型以及具体值:\n <class 'torch.Tensor'> tensor([1., 2., 3.])\n变量的类型以及具体的值:\n <class 'torch.Tensor'> tensor([1., 2., 3.], requires_grad=True)\n"
    }
   ],
   "source": [
    "# 然后定义pytorch中的tensor 并将tensor转化成Variable的形式\n",
    "x_tensor = torch.FloatTensor([1, 2, 3])\n",
    "print('张量的类型以及具体值:\\n', type(x_tensor), x_tensor)\n",
    "x_var = Variable(x_tensor, requires_grad = True)\n",
    "print('变量的类型以及具体的值:\\n', type(x_var), x_var)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nh:  tensor([1.3333, 2.6667, 4.0000], grad_fn=<MulBackward0>) <MulBackward0 object at 0x12c1613d0>\n\nh.backward:  tensor([1., 2., 3.], requires_grad=True) tensor([4.0000, 6.6667, 9.3333]) None\n"
    }
   ],
   "source": [
    "# 显示的调用pytorch中的反向传播\n",
    "\n",
    "h = 2 * x_var / 3 * 2\n",
    "print('\\nh: ', h, h.grad_fn)\n",
    "h.backward(torch.FloatTensor([3, 5, 7]))\n",
    "print('\\nh.backward: ', x_var, x_var.grad, x_var.grad_fn)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\ny： tensor([2., 3., 4.], grad_fn=<AddBackward0>) \n <AddBackward0 object at 0x12d431910>\n\nx_var： tensor([ 6.0000,  8.6667, 11.3333]) \n None\n\nh： tensor([3., 4., 5.], grad_fn=<AddBackward0>) \n <AddBackward0 object at 0x12d431910>\n\nx_var： tensor([ 8.0000, 10.6667, 13.3333]) \n None\n"
    }
   ],
   "source": [
    "# 显示的调用pytorch中的反向传播\n",
    "y = x_var + 1\n",
    "h = x_var + 2\n",
    "\n",
    "y.backward(torch.FloatTensor([2, 2, 2]))\n",
    "print('\\ny：', y, '\\n', y.grad_fn)\n",
    "print('\\nx_var：', x_var.grad, '\\n', x_var.grad_fn)\n",
    "\n",
    "print('\\nh：', h, '\\n', h.grad_fn)\n",
    "h.backward(torch.FloatTensor([2, 2, 2]))\n",
    "print('\\nx_var：', x_var.grad, '\\n', x_var.grad_fn)"
   ]
  }
 ]
}